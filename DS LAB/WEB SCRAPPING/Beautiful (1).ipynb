{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import necessary libraries:\n",
    "   - `requests`: Used to send HTTP requests to the webpage.\n",
    "   - `BeautifulSoup`: A library for parsing HTML content and navigating the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the target URL: The script specifies the URL of the Wikipedia Main Page.\n",
    "\n",
    "3. Send an HTTP GET request to the URL using the `requests.get(url)` method and store the response in the `response` variable.\n",
    "\n",
    "4. Check the response status using `print(response)`. This is just to verify that the request was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create a BeautifulSoup object to parse the HTML content of the page. `html_content` stores the raw HTML text, and `soup` is the BeautifulSoup object.\n",
    "6. Use `soup.prettify()` to make the HTML content more readable, although this step is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "soup.prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Extract Featured Articles:\n",
    "   - Use `soup.find_all(\"div\", class_=\"mp-featured-article\")` to locate all elements with the specified class that represent featured articles.\n",
    "   - Iterate through the found elements, find the article title using `.find(\"h2\").text`, and print each title.\n",
    "\n",
    "8. Extract \"Did You Know\" items:\n",
    "   - Use `soup.find(\"div\", id=\"mp-dyk\")` to locate the section containing \"Did You Know\" items.\n",
    "   - Iterate through the list items within this section using `.find_all(\"li\")`, and print each item's text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Example: Extracting all the featured article titles\n",
    "featured_articles = soup.find_all(\"div\", class_=\"mp-featured-article\")\n",
    "\n",
    "for article in featured_articles:\n",
    "    title = article.find(\"h2\").text\n",
    "    print(title)\n",
    "\n",
    "# Example: Extracting the \"Did you know\" section\n",
    "did_you_know = soup.find(\"div\", id=\"mp-dyk\")\n",
    "\n",
    "for item in did_you_know.find_all(\"li\"):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Error Handling:\n",
    "   - Before sending the request, a try-except block is used to catch any potential HTTP errors using `requests.exceptions.HTTPError`.\n",
    "\n",
    "10. Exception Handling:\n",
    "   - Inside the try-except block, there's another try-except block to handle possible attribute errors that may occur when parsing the HTML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "except requests.exceptions.HTTPError as error:\n",
    "    print(error)\n",
    "    exit()\n",
    "\n",
    "html_content = response.text\n",
    "\n",
    "try:\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    featured_articles = soup.find_all(\"div\", class_=\"mp-featured-article\")\n",
    "    for article in featured_articles:\n",
    "        title = article.find(\"h2\").text\n",
    "        print(title)\n",
    "\n",
    "    did_you_know = soup.find(\"div\", id=\"mp-dyk\")\n",
    "    for item in did_you_know.find_all(\"li\"):\n",
    "        print(item.text)\n",
    "except AttributeError as error:\n",
    "    print(error)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Saving Data:\n",
    "   - The extracted data, including featured article titles and \"Did You Know\" items, is stored in a Python dictionary called `data`.\n",
    "   - This data is then saved as a JSON file named \"data.json\" using the `json.dump(data, outfile)` method.\n",
    "   \n",
    "12. The script includes proper error handling and exit statements to gracefully handle HTTP errors and attribute errors during the web scraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "except requests.exceptions.HTTPError as error:\n",
    "    print(error)\n",
    "    exit()\n",
    "\n",
    "html_content = response.text\n",
    "\n",
    "try:\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    featured_articles = soup.find_all(\"div\", class_=\"mp-featured-article\")\n",
    "    article_titles = [article.find(\"h2\").text for article in featured_articles]\n",
    "\n",
    "    did_you_know = soup.find(\"div\", id=\"mp-dyk\")\n",
    "    items = [item.text for item in did_you_know.find_all(\"li\")]\n",
    "\n",
    "    data = {\"featured_articles\": article_titles, \"did_you_know\": items}\n",
    "\n",
    "    with open(\"data.json\", \"w\") as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "except AttributeError as error:\n",
    "    print(error)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethical web scraping involves responsible and respectful behavior towards websites, their data, and their users. It's crucial to strike a balance between extracting valuable information and respecting the rights and wishes of website owners and data subjects.\n",
    "\n",
    "Respecting Robots.txt:\n",
    "\n",
    "Always check a website's robots.txt file to ensure you are allowed to scrape the data. Respect the rules specified in this file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
