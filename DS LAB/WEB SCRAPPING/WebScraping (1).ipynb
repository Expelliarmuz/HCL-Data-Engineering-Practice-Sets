{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the necessary libraries (`requests` and `BeautifulSoup`) by running `!pip install beautifulsoup4 requests`. This command installs these libraries using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install BeautifulSoup and Requests\n",
    "!pip install beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Import the `requests` library to send HTTP requests.\n",
    "\n",
    "3. Send an HTTP GET request to the URL `https://example.com` using `requests.get(url)` and store the response in the `response` variable.\n",
    "\n",
    "4. Display the HTML content of the response using `print(response.text)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Send an HTTP GET / POST/PUT/DELETE request\n",
    "url = 'https://example.com'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Display the HTML content\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Import the `BeautifulSoup` library from `bs4` to parse the HTML content.\n",
    "\n",
    "6. Create a BeautifulSoup object `soup` by passing the HTML content (`response.text`) and specifying the parser ('html.parser').\n",
    "\n",
    "Navigating the HTML Tree:\n",
    "\n",
    "Use soup.find() to find the first occurrence of an element.\n",
    "Use soup.find_all() to find all occurrences of a specific element.\n",
    "Navigate the HTML tree using methods like .parent, .contents, .next_sibling, and .previous_sibling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find and print specific elements from the page\n",
    "title = soup.title.text\n",
    "print('Title:', title)\n",
    "\n",
    "# Find all links on the page and print their href attributes\n",
    "links = soup.find_all('a')\n",
    "for link in links:\n",
    "    print('Link:', link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. The script sends an HTTP GET request to 'http://quotes.toscrape.com/' and parses the HTML content using BeautifulSoup.\n",
    "\n",
    "8. It then extracts the text of all <span> elements with the class 'text' (quotes) and all <small> elements with the class 'author' (authors) from the parsed HTML.\n",
    "\n",
    "The extracted quotes and authors are stored in the quotes and authors lists.\n",
    "\n",
    "Finally, it prints each quote and its corresponding author in a loop.\n",
    "\n",
    "This script will correctly extract and print the quotes and authors from the specified URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Sample HTML content (can be fetched using requests library in a real scenario)\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Book List</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>List of Books</h1>\n",
    "    <ul class=\"book-list\">\n",
    "        <li class=\"book\" id=\"book1\">Book 1</li>\n",
    "        <li class=\"book\" id=\"book2\">Book 2</li>\n",
    "        <li class=\"book\" id=\"book3\">Book 3</li>\n",
    "    </ul>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Access and print the book titles\n",
    "book_list = soup.find('ul', class_='book-list')\n",
    "book_titles = book_list.find_all('li', class_='book')\n",
    "\n",
    "for book in book_titles:\n",
    "    print('Book Title:', book.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Send an HTTP GET request\n",
    "url = ' http://quotes.toscrape.com/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Scraping quotes and authors\n",
    "quote_elements = soup.find_all('span', class_='text')\n",
    "author_elements = soup.find_all('small', class_='author')\n",
    "\n",
    "quotes = [quote.text for quote in quote_elements]\n",
    "authors = [author.text for author in author_elements]\n",
    "print(quotes,\"\\n\\n\",authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Import the `csv` module to work with CSV files.\n",
    "\n",
    "10. Create and open a new CSV file named 'quotes.csv' in write mode with the specified fieldnames ('Quote' and 'Author'). Use the `csv.DictWriter` class to create a CSV writer.\n",
    "\n",
    "11. Write the header row using `writer.writeheader()`.\n",
    "\n",
    "12. Iterate through the quotes and authors extracted earlier and write each pair to the CSV file using `writer.writerow()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Create and write to a CSV file\n",
    "with open('quotes.csv', 'w', newline='') as csvfile:\n",
    "    field = ['Quote', 'Author']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for i in range(len(quotes)):\n",
    "        writer.writerow({'Quote': quotes[i], 'Author': authors[i]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethical web scraping involves responsible and respectful behavior towards websites, their data, and their users. It's crucial to strike a balance between extracting valuable information and respecting the rights and wishes of website owners and data subjects.\n",
    "\n",
    "Respecting Robots.txt:\n",
    "\n",
    "Always check a website's robots.txt file to ensure you are allowed to scrape the data. Respect the rules specified in this file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
